{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce78009",
   "metadata": {},
   "source": [
    "<img src='../images/bildungscampus_logo.png' width=\"40%\" align=\"left\" />\n",
    "<img src='../images/hhn.png' width=\"25%\" align=\"right\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3085bd38",
   "metadata": {},
   "source": [
    "# Schritt 2: Intent Classification\n",
    "Masterarbeit - Sebastian Kahlert | Fakultät Wirtschaft und Verkehr | Wirtschaftsinformatik - Informationsmanagement und Data Science | WS 2021/22\n",
    "\n",
    "<img src='../images/bar.png'/>\n",
    "\n",
    "## Implementation und Training des Neuronales Netzes dür die Klassifizerung von Intents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c3981",
   "metadata": {},
   "source": [
    "<img src='../images/neural_net.png' align=\"right\" />\n",
    "\n",
    "Abbildung: Vereinfachte Darstellung des Feddforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fdfadd",
   "metadata": {},
   "source": [
    "Zur Klassifizierung der Intents wird ein **Feedforward Neural Network** verwendet. Es ist eines der einfachsten Arten von Nueronalen Netzen, da die Daten hier nur in eine Richtung fließen. Da der Input des Netzes im Rahmen dieser Arbeit Texte sind, müssen diese im Vorfeld in Zahlen umgewandelt werden, da Neuronale Netze keine Buchstaben bzw. Sätze und Wörter als Input verarbeiten können. Aus diesem Grund werden die Trainingsdaten und die Eingaben des Benutzers durch einen **Bag  of Words Ansatz** umgewandelt, bevor diese vom Neuronalen Netzwerk verarbeitet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d24b66",
   "metadata": {},
   "source": [
    "### 2.1. Import benötigter Bibliotheken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56330739",
   "metadata": {},
   "source": [
    "Zur Vorbereitung der Trainingsdaten wird unteranderem der **Stemmer von NLTK** verwendet. **TensorFlow** ist ein Framework zur datenstromorientierten Programmierung und wird für die Implementation des neuronalen Netzes verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a44f6e1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T12:25:57.563825Z",
     "start_time": "2022-03-04T12:25:57.553854Z"
    }
   },
   "outputs": [],
   "source": [
    "# import of necessary libraries\n",
    "import nltk \n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy\n",
    "import tflearn \n",
    "import tensorflow\n",
    "import random \n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714111be",
   "metadata": {},
   "source": [
    "### 2.2. Trainingsdaten vorbereiten, Implementation und Training des Neuronalen Netzes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4c93d",
   "metadata": {},
   "source": [
    "Die Trainingsdaten liegen in einer json-Datei vor. Die folgende Funktion öffnet die Datei, extrahiert die Daten und speichert diese in verschiedenen Listen. Anschließend werden diese durch Bag of Words in lesbare Trainingsdaten umgewandelt. Zum Schluss wird das Neuronale Netz erstellt, trainert und gespeichert, sodass es auch in anderen Notebooks geladen werden kann."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad57862",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T15:05:52.999141Z",
     "start_time": "2022-02-27T15:05:52.993118Z"
    }
   },
   "source": [
    "<img src='../images/training_data.jpg' align=\"right\" />\n",
    "Ausschnit aus den Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf45ab0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T12:25:59.526955Z",
     "start_time": "2022-03-04T12:25:59.513380Z"
    }
   },
   "outputs": [],
   "source": [
    "# open and load json file with patterns --> questions and tags\n",
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cde3cb7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T12:29:00.796361Z",
     "start_time": "2022-03-04T12:29:00.470792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Sebi\\OneDrive\\Studium\\Thesis_Chatbot\\2. Intent Classififcation\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "# prepare training data + implement and train Neural Net\n",
    "try: \n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except:\n",
    "    words = [] #all word in the json file\n",
    "    labels = [] #all intents in the json file\n",
    "    docs_x = [] #input of user\n",
    "    docs_y = [] #intent for each input\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            #Stemming - Getting the root words with tokenize\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    # Neural Network versteht nur Nummern --> Bag of words\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag =  []\n",
    "        wrds = [stemmer.stem(w) for w in doc]\n",
    "\n",
    "        for w in words: \n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else: \n",
    "                bag.append(0)\n",
    "\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "    # in array für model\n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)\n",
    "    \n",
    "    with open(\"data.pickle\", \"wb\") as f:\n",
    "        pickle.dump((words, labels, training, output), f)\n",
    "        \n",
    "#create, train and save neural net\n",
    "tensorflow.compat.v1.reset_default_graph()\n",
    "    \n",
    "# Creating the Neural Network\n",
    "net = tflearn.input_data(shape= [None, len(training[0])]) #input layer Neurons = numer of words in training\n",
    "net = tflearn.fully_connected(net, 8) #hidden layer fully connected with 8 neuron\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\" ) #output layer 6 Neurons = labels\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "if os.path.exists(r\"model.tflearn.meta\"):\n",
    "    model.load(r\"model.tflearn\")\n",
    "else:\n",
    "    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "    model.save(r\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b199fb7f",
   "metadata": {},
   "source": [
    "### 2.3. Herausforderungen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c508a",
   "metadata": {},
   "source": [
    "* Es existieren zwar Trainingsdatensätze für die Intent Classification, jedoch nicht für diesen speziellen Use Case. Das heißt die Trainingsdaten müssen zum großen Teil manuell erstellt werden.\n",
    "* Es kann schwierig sein vorherzusehen, wie genau der Benutzer mit dem Chatbot sprechen wird. Demnach kann es notwendig sein, in Laufe der Zeit die Trianingsdaten anzupassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690f8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prototype",
   "language": "python",
   "name": "prototype"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
